<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
body {
  margin: 0;
  font-family: Arial, Helvetica, sans-serif;
}

.topnav {
  overflow: hidden;
  background-color: #333;
}

.topnav a {
  float: left;
  color: #f2f2f2;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 17px;
}

.topnav a:hover {
  background-color: #ddd;
  color: black;
}

.topnav a.active {
  background-color: #04AA6D;
  color: white;
}
</style>
</head>
<body>

<div class="topnav">
  <h1 style="text-align:center; color:white;">Interpreting CTML to design a platform for Multimedia Music Learning</h1>
  </br>
  <a href="index.html">Introduction</a>
  <a href="definitions.html">Definitions</a>
  <a href="goals.html">Goals</a>
  <a href="specifications.html">Specifications</a>
  <a href="dualcoding.html">CTML Dual-Coding Theory</a>
  <a class="active" href="principles.html">Interpreting CTML Principles</a>
  <a href="design-choices.html">Design Choices</a>
  <a href="mockups.html">Mockups</a>
  <a href="references.html">References</a>
  <p style="text-align:right; color:white; padding-right:25px">by Josh Milton</p>
</div>

<div style="padding-left:10%; padding-right:10%; text-align:left;">

  <h1 style ="text-align: center;">Interpreting CTML Principles</h1>
  <p>Now that we have discussed how music may fit within the Dual-Coding model (See page: <a href="dualcoding.html">CTML Dual-Coding Theory</a>) we can use the assumptions we have made to interpret the principles of CTML as they might apply to music instruction.</p>
  <p>Before trying to interpret each of these principles we must first take a step back and try to understand how music, specifically audio and notation stimuli, relates to the Dual-Coding foundations of CTML. As well as adjacent theories like Cognitive Learning Theory (Sweller, 1998). </p>
  <br>
  <br>
  <h2 style ="text-align: center;">Dual-Coding Theory?</h2>
  <p>Looking again at Mayer’s Dual-Coding Theory (See Figure 5), we can see that no input is accounted for within the auditory processing channel other than that of spoken word – the role of musical sound within CTML is typically relegated to that of extraneous background music (Moreno & Mayer, 2000) and thus has not been considered as an essential stimulus.</p>
  <div align = "center";>  <img src="img/Fig5.png">
    <p><b>Figure 5:</b> The CTML Dual-Coding Theory (Mayer, 2014)</p>
    </div>
    <br>
    <h2>Aural Stimuli in Music</h2>
  <p>Peretz (2001) suggests that entirely distinct components of the brain are used for processing musical cues than verbal information, indicating that these can be discriminated aurally by trained musicians within the selection and organisation phases of the working and sensory memories. This idea that music is processed discretely from verbal sound is supported by literature on working memory and cognitive load relating to music (Berz, 1995; Owens & Sweller, 2008), though still contributing to overall cognitive load. This is not necessarily to the detriment of instruction involving spoken word and music, as Ahmad Aldalalah and Fook Fong (2010) suggest that total cognitive load and working memory capacity may be higher in those with musical training.</p>
  <br>
  <div align = "center";>  <img src="img/Fig6.png">
    <p><b>Figure 6:</b> A distinct Music model alongside the Verbal model in a larger Aural model of working memory</p>
  </div>
  <br>
  <p>Figure 6 showcases the ideas of Peretz (2001), Berz (1995) and Owens and Sweller (2008) as applied to the CTML Dual-Coding model, with separation of musical cues processed separately in working memory before being organised into a discrete Music Model within a larger Aural model.</p>
<br>
<br>
  <h2>Visual Stimuli in Music</h2>
  <p>Sergent et al. (1992) describes the phenomenon of visual musical information, specifically musical notation, being subconsciously processed aurally by trained musicians, meaning that visual stimulus may inherently be passed over into the musical processing model as well as the pictorial model in processing music notation. </p>
  <p>While musical notation syntax is structured in some similar ways to how verbal language is structured, processing of this is typically much more complex than a written language system (Sergent et al., 1992; Fourie, 2004) owing to the multiple dimensions in which it is encoded, e.g. pitch and rhythm (Fourie, 2004) and thus may introduce a higher level of cognitive load than verbal information. </p>
  <br>
  <p>Mapping the audiation of visual musical stimulus to the above model (Figure 6) might look something like Figure 7 (below), creating an inherent dual-modality principle when music notation is presented to trained musicians.</p>
  <div align = "center";>  <img src="img/Fig7.png">
    <p><b>Figure 7:</b> A distinct Music model showcasing interactions between visual and aural music stimuli</p>
  </div>
<br>
<br>

</div>

</body>
</html>